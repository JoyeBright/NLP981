{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGNS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsAbi-i8v74c",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Word Embedding**\n",
        "\n",
        "> **Word2Vec, Continuous Bag of Word (CBOW)**\n",
        "\n",
        "> **Word2Vec, Skip-gram with negative sampling (SGNS)**\n",
        "\n",
        "> **Main key point: Distributional Hypothesis**\n",
        "\n",
        "> Goal: Predict the context words from a given word\n",
        "\n",
        "# **How to implement SGNS Algorithm:**\n",
        "\n",
        "\n",
        "1.   Data preprocessing\n",
        "2.   Hyperparameters\n",
        "3.   Training Data\n",
        "4.   Model Fitting\n",
        "5.   Inference/Prediction the testing samples\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sT6SltefXAJf"
      },
      "source": [
        "### **Main Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdXqCMgWYYtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "class word2vec():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = hyperparameters['n']\n",
        "    self.learningrate = hyperparameters['learning_rate']\n",
        "    self.epochs = hyperparameters['epochs']\n",
        "    self.windowsize = hyperparameters['window_size']\n",
        "\n",
        " \n",
        "  def word2onehot(self, word):\n",
        "    word_vector =  np.zeros(self.vocabulary_count)\n",
        "    word_index = self.word_index[word]\n",
        "    word_vector[word_index] = 1\n",
        "    return word_vector\n",
        "\n",
        "  def generate_training_data(self, setting, corpus):\n",
        "    word_counts = defaultdict(int)\n",
        "    # print(word_counts)\n",
        "    for row in corpus:\n",
        "      for token in row:\n",
        "        word_counts[token] +=1 \n",
        "    #print(word_counts)\n",
        "    self.vocabulary_count = len(word_counts.keys())\n",
        "    #print(self.vocabulary_count)\n",
        "    self.words_list = list(word_counts.keys())\n",
        "    #print(self.words_list)\n",
        "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
        "    #print(self.word_index)\n",
        "    self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
        "    #print(self.index_word)\n",
        "\n",
        "    training_data = []\n",
        "    for sentence in corpus:\n",
        "      sentence_length = len(sentence)\n",
        "      for i , word in enumerate(sentence):\n",
        "        word_target = self.word2onehot(sentence[i])\n",
        "        #print(word_target)\n",
        "        word_context = []\n",
        "        for j in range(i - self.windowsize, i + self.windowsize + 1):\n",
        "          if j !=i and  j <= sentence_length - 1 and j >= 0:\n",
        "            word_context.append(self.word2onehot(sentence[j]))\n",
        "            # print(word_context)\n",
        "        training_data.append([word_target, word_context])\n",
        "                              \n",
        "      return np.array(training_data)\n",
        "    \n",
        "  def model_training(self, training_data):\n",
        "      self.w1 = np.random.uniform(-1, 1, (self.vocabulary_count, self.n))\n",
        "      self.w2 = np.random.uniform(-1, 1, (self.n, self.vocabulary_count))\n",
        "      for i in range(0, self.epochs):\n",
        "        # self.loss = 0\n",
        "        for word_target, word_context in training_data:\n",
        "          h, u, y_pred= self.forward_pass(word_target)\n",
        "          # print(y_pred)\n",
        "   \n",
        "  def forward_pass(self, x):\n",
        "      h = np.dot(self.w1.T, x)\n",
        "      u = np.dot(self.w2.T, h)\n",
        "      y_pred= self.softmax(u)\n",
        "      return h, u, y_pred\n",
        "    \n",
        "    \n",
        "  def softmax(self, x):\n",
        "      e = np.exp(x - np.max(x))\n",
        "      return e / e.sum(axis=0)\n",
        "\n",
        "  def word_vector(self, word):\n",
        "    word_index = self.word_index[word]\n",
        "    word_vector = self.w1[word_index]\n",
        "    return word_vector\n",
        "\n",
        "  def similar_vectors(self, word, n):\n",
        "    vw1 = self.word_vector(word)\n",
        "    word_similar={}\n",
        "    for i in range(self.vocabulary_count):\n",
        "      vw2 = self.w1[i]\n",
        "      theta_nom= np.dot(vw1, vw2)\n",
        "      theta_denom = np.linalg.norm(vw1) * np.linalg.norm(vw2)\n",
        "      theta = theta_nom / theta_denom\n",
        "      # print(theta)\n",
        "\n",
        "      word = self.index_word[i]\n",
        "      word_similar[word] = theta\n",
        "    # {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n",
        "    words_sorted = sorted(word_similar.items(), key=lambda ss: ss[1], reverse=True)\n",
        "    for word, similar in words_sorted[:n]:\n",
        "      print(word, similar)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDL8ZdH4Q6k",
        "colab_type": "text"
      },
      "source": [
        "### **1.Data PreProcessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJayMXuIvN9t",
        "colab_type": "code",
        "outputId": "7b69dd10-0ecf-4775-b829-395c4470becf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define the mini corpus\n",
        "document = \"A combination of Machine Learning and Natural Language Processing works well\"\n",
        "\n",
        "# Tokenizing and build a vocabulary\n",
        "corpus = [[]]\n",
        "for token in document.split():\n",
        "  corpus[0].append(token.lower())\n",
        "\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['a', 'combination', 'of', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'works', 'well']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vNBazTV56jb",
        "colab_type": "text"
      },
      "source": [
        "### **2. Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDkb0Wt76Eu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters = {\n",
        "    'window_size': 2, #it covers two words left and two words right\n",
        "    'n': 11, # dimension of word embedding\n",
        "    'epochs': 40, # number of training epochs\n",
        "    'learning_rate': 0.01, # a coefficient for updating weights\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NdgeMkppPrpk"
      },
      "source": [
        "### **3. Generate Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_TIPfRTPwC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need to create one-hot vector based on our given corpus\n",
        "# 1 [target(a)], [context(combination, of)] == [10000000000],[01000000000][00100000000]\n",
        "# instance\n",
        "w2v = word2vec()\n",
        "\n",
        "training_data = w2v.generate_training_data(hyperparameters, corpus)\n",
        "# print(training_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8p6wS1WHWWuA"
      },
      "source": [
        "### **4. Model Training**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcgQI-6oWTvL",
        "colab_type": "code",
        "outputId": "534a20a9-c46d-4814-9ce9-f35152f7dfda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "w2v.model_training(training_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.08779638 0.07481873 0.02720081 0.07741055 0.00744272 0.16597957\n",
            " 0.02244375 0.03065301 0.24696449 0.05698785 0.20230215]\n",
            "[0.05535867 0.00701134 0.03824704 0.10045965 0.56325283 0.01725363\n",
            " 0.0265237  0.05966504 0.09830109 0.0269263  0.00700071]\n",
            "[0.04181769 0.01066172 0.11196532 0.21611837 0.09291857 0.06353542\n",
            " 0.12186928 0.09201719 0.00719568 0.10265939 0.13924137]\n",
            "[0.01979228 0.67167761 0.0380796  0.00334896 0.01721451 0.03192899\n",
            " 0.10938238 0.05316565 0.02946898 0.01481168 0.01112936]\n",
            "[0.08594459 0.01955307 0.03806679 0.20510115 0.00741567 0.1290254\n",
            " 0.00654433 0.01746104 0.087872   0.22842949 0.17458647]\n",
            "[0.09563497 0.0609889  0.12708249 0.11587498 0.02070406 0.07517313\n",
            " 0.07438113 0.10863157 0.08416487 0.03121457 0.20614931]\n",
            "[0.05032016 0.23525726 0.16200512 0.01933368 0.09044005 0.02026146\n",
            " 0.06624078 0.18744993 0.0542594  0.08477761 0.02965456]\n",
            "[0.09318229 0.04413759 0.24420036 0.10517933 0.12382943 0.06460056\n",
            " 0.0371188  0.0105303  0.0077964  0.15646752 0.11295743]\n",
            "[0.0451643  0.10487824 0.08784491 0.03077638 0.04817766 0.0241796\n",
            " 0.07871515 0.36046298 0.03539558 0.05103012 0.13337506]\n",
            "[0.03536469 0.38921382 0.07153202 0.01173604 0.02046491 0.14331057\n",
            " 0.04427569 0.01477941 0.02839212 0.10029694 0.14063378]\n",
            "[0.03581558 0.11027609 0.07132603 0.0326665  0.0393842  0.05157923\n",
            " 0.29849825 0.26142471 0.03815447 0.02041243 0.0404625 ]\n",
            "[0.08779638 0.07481873 0.02720081 0.07741055 0.00744272 0.16597957\n",
            " 0.02244375 0.03065301 0.24696449 0.05698785 0.20230215]\n",
            "[0.05535867 0.00701134 0.03824704 0.10045965 0.56325283 0.01725363\n",
            " 0.0265237  0.05966504 0.09830109 0.0269263  0.00700071]\n",
            "[0.04181769 0.01066172 0.11196532 0.21611837 0.09291857 0.06353542\n",
            " 0.12186928 0.09201719 0.00719568 0.10265939 0.13924137]\n",
            "[0.01979228 0.67167761 0.0380796  0.00334896 0.01721451 0.03192899\n",
            " 0.10938238 0.05316565 0.02946898 0.01481168 0.01112936]\n",
            "[0.08594459 0.01955307 0.03806679 0.20510115 0.00741567 0.1290254\n",
            " 0.00654433 0.01746104 0.087872   0.22842949 0.17458647]\n",
            "[0.09563497 0.0609889  0.12708249 0.11587498 0.02070406 0.07517313\n",
            " 0.07438113 0.10863157 0.08416487 0.03121457 0.20614931]\n",
            "[0.05032016 0.23525726 0.16200512 0.01933368 0.09044005 0.02026146\n",
            " 0.06624078 0.18744993 0.0542594  0.08477761 0.02965456]\n",
            "[0.09318229 0.04413759 0.24420036 0.10517933 0.12382943 0.06460056\n",
            " 0.0371188  0.0105303  0.0077964  0.15646752 0.11295743]\n",
            "[0.0451643  0.10487824 0.08784491 0.03077638 0.04817766 0.0241796\n",
            " 0.07871515 0.36046298 0.03539558 0.05103012 0.13337506]\n",
            "[0.03536469 0.38921382 0.07153202 0.01173604 0.02046491 0.14331057\n",
            " 0.04427569 0.01477941 0.02839212 0.10029694 0.14063378]\n",
            "[0.03581558 0.11027609 0.07132603 0.0326665  0.0393842  0.05157923\n",
            " 0.29849825 0.26142471 0.03815447 0.02041243 0.0404625 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q70mqOvCGGmn"
      },
      "source": [
        "### **5. Model Prediction**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x8Nb-IAGI1a",
        "colab_type": "code",
        "outputId": "d59629e9-4526-4bc6-8255-adaac59b359e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vector = w2v.word_vector(\"works\")\n",
        "print(vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.5965974   0.59358364  0.49175356  0.59782454 -0.10149338  0.5909372\n",
            " -0.4941789   0.73069452 -0.13549471 -0.7486393   0.16786503]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rhoWqveJIVz4"
      },
      "source": [
        "### **Finding Similar Words**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2QdR0FVIeNN",
        "colab_type": "code",
        "outputId": "2e60a427-6834-4772-fe91-151a0fbea3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "w2v.similar_vectors(\"works\", 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "works 1.0\n",
            "language 0.34217254302544925\n",
            "machine 0.20539544566784484\n",
            "natural 0.16382679527923805\n",
            "a 0.13091314242232238\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}